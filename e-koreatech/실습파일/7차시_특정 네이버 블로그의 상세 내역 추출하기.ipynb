{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      " 네이버 블로그 상세 내역과 이미지 정보 수집하기\n",
      "================================================================================\n",
      "1.크롤링할 블로그 주소를 입력하세요: https://blog.naver.com/hy820715/221514204265\n",
      "요청하신 데이터를 수집 중입니다\n",
      "\n",
      "\n",
      "잠시만 기다려 주세요~~~~~^^\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "데이터 추출 시작합니다 =================================================\n",
      "\n",
      "\n",
      "\n",
      "요청하신 블로그는 유형 2형이고 해당 블로그 정보를 수집합니다~~~~~~~~~~\n",
      "1.블로그주소:  https://blog.naver.com/hy820715/221514204265\n",
      "2.작성자 닉네임:  가치랩장입니다\n",
      "3.작성일자: 2019. 4. 15. 16:27\n",
      "4.블로그내용: \n",
      " 웹 크롤링 진짜 많이 재미있죠?웹 크롤링에 대한 다양한 예제는 바로 이 책에 들어 있어요~ 이 책안에는 다양한 유형의 웹사이트를 파이썬과 셀레니움을 활용하여 수집하는 노하우들이 다 들어 있습니다~하나씩 하나씩 따라하다 보면 금방 실력자가 되어 있으실 거예요~~^^​그리고 데이터베이스에 데이터를 저장한 후 조회하여 분석하는 경우도 많아서 많은 기업들이 데이터분석가나 인공지능 관련 분석가를 찾을 때SQL 언어 사용 능력도 많이 요구합니다~~SQL 관련 내용은 아래의 책을 보시면 완전 쉽지만 실무에서 많이 사용하는 다양한 기법들을 배우실 수 있을거예요~~[ 오라클 기반 SQL 책 ] [ MySQL 기반 책 ] ​무엇보다도 절대로 포기하지 말고 열공하는 자세가 가장 중요합니다~~^^열공해 주세요~~^^​가치랩장 드림.​​​ \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 네이버 블로그 상세 정보 추출하기\n",
    "# 연습용 블로그 : https://blog.naver.com/hy820715/221514204265\n",
    "print(\"=\" *80)\n",
    "print(\" 네이버 블로그 상세 내역과 이미지 정보 수집하기\")\n",
    "print(\"=\" *80)\n",
    "\n",
    "#Step 1. 필요한 모듈과 라이브러리를 로딩합니다.\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import time\n",
    "import sys\n",
    "import math\n",
    "import numpy \n",
    "import pandas as pd  \n",
    "\n",
    "#Step 2. 사용자로부터 필요한 정보 입력받기\n",
    "blog_addr = input('1.크롤링할 블로그 주소를 입력하세요: ')\n",
    "\n",
    "print(\"요청하신 데이터를 수집 중입니다\")\n",
    "print(\"\\n\")\n",
    "print(\"잠시만 기다려 주세요~~~~~^^\")\n",
    "print(\"\\n\")\n",
    "    \n",
    "#Step 3. 크롬 드라이버를 사용해서 웹 브라우저를 실행합니다.\n",
    "\n",
    "s_time = time.time( )\n",
    "\n",
    "s = Service(\"c:/py_temp/chromedriver.exe\")\n",
    "driver = webdriver.Chrome(service=s)\n",
    "\n",
    "#Step 4. 각 블로그의 상세 결과를 출력하여 파일에 저장하기\n",
    "blog_addr2=[]\n",
    "w_name2=[]\n",
    "w_date2=[]\n",
    "blog_txt2=[]\n",
    "\n",
    "gubun = blog_addr.split(\"/\")\n",
    "\n",
    "if gubun[2] != 'blog.naver.com' :\n",
    "    print(\" 네이버 블로그만 가능합니다\")\n",
    "    \n",
    "else :\n",
    "    driver.get(blog_addr)\n",
    "    driver.maximize_window( )\n",
    "    time.sleep(random.randrange(2,5))  # 2 - 5 초 사이에 랜덤으로 시간 선택\n",
    "      \n",
    "    print(\"\\n\")\n",
    "    print(\"데이터 추출 시작합니다 =================================================\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # iframe 전환하기\n",
    "    driver.switch_to.frame('mainFrame')\n",
    "    \n",
    "    #전체 소스코드 가져오기\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    addr_1 = soup.select('div[class=\"se_component_wrap sect_dsc __se_component_area\"]')   \n",
    "    addr_2 = soup.select('div[class=\"se-main-container\"]')  \n",
    "\n",
    "    if addr_1 :\n",
    "        img_no =  1\n",
    "        print()\n",
    "        print(\"요청하신 블로그는 유형 1형이고 해당 블로그 정보를 수집합니다~~~~~~~~~~\")\n",
    "        \n",
    "        # 블로그 주소\n",
    "        print(\"1.블로그주소: \",blog_addr)\n",
    "        blog_addr2.append(blog_addr)\n",
    "        \n",
    "        # 작성자 닉네임\n",
    "        writer = soup.select(\"div.blog2_container > span.writer\")\n",
    "        try :\n",
    "            wname = writer[0].get_text( )   # 작성자 닉네임\n",
    "        except IndexError :\n",
    "            wname = \"작성자 닉네임이 없습니다\"\n",
    "        else :\n",
    "            wname = wname.replace(\"\\n\",\"\")\n",
    "\n",
    "        print(\"2.작성자 닉네임: \",wname )\n",
    "        w_name2.append(wname)\n",
    "        \n",
    "        # 작성일자\n",
    "        wdate = soup.select(\"div.blog2_container > span.se_publishDate.pcol2\")\n",
    "        try : \n",
    "            wdate = wdate[0].get_text( )\n",
    "        except IndexError :\n",
    "            wdate = ' '\n",
    "\n",
    "        print(\"3.작성일자:\",wdate)\n",
    "        w_date2.append(wdate)\n",
    "\n",
    "        # 블로그 본문 내용\n",
    "        for i in addr_1:\n",
    "            blog_txt = i.text.replace(\"\\n\",\"\")\n",
    "            print(\"4.블로그내용: \\n\",blog_txt) \n",
    "            print(\"\\n\")\n",
    "            blog_txt2.append(blog_txt)\n",
    "            \n",
    "    elif addr_2 :\n",
    "        img_no = 1\n",
    "        print()\n",
    "        print(\"요청하신 블로그는 유형 2형이고 해당 블로그 정보를 수집합니다~~~~~~~~~~\")\n",
    "        \n",
    "        # 블로그 주소\n",
    "        print(\"1.블로그주소: \",blog_addr)\n",
    "        blog_addr2.append(blog_addr)\n",
    "        \n",
    "        # 작성자 닉네임\n",
    "        writer = soup.select(\"div.blog2_container > span.writer\")\n",
    "        try :\n",
    "            wname = writer[0].get_text( )   # 작성자 닉네임\n",
    "        except IndexError :\n",
    "            wname = \"작성자 닉네임이 없습니다\"\n",
    "        else :\n",
    "            wname = wname.replace(\"\\n\",\"\")\n",
    "\n",
    "        print(\"2.작성자 닉네임: \",wname )\n",
    "        w_name2.append(wname)\n",
    "        \n",
    "        # 작성일자\n",
    "        wdate = soup.select(\"div.blog2_container > span.se_publishDate.pcol2\")\n",
    "        try : \n",
    "            wdate = wdate[0].get_text( )\n",
    "        except IndexError :\n",
    "            wdate = ' '\n",
    "\n",
    "        print(\"3.작성일자:\",wdate)\n",
    "        w_date2.append(wdate)\n",
    "        \n",
    "        #블로그 본문 내용\n",
    "        for i in addr_2:\n",
    "            blog_txt = i.text.replace(\"\\n\",\"\")\n",
    "            print(\"4.블로그내용: \\n\",blog_txt) \n",
    "            print(\"\\n\")\n",
    "            blog_txt2.append(blog_txt)\n",
    "                \n",
    "        \n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 구 주소 블로그 : https://blog.naver.com/junhyuk_abba/221286810022  - postViewArea 클래스명 사용\n",
    "# 신 주소 블로그 : https://blog.naver.com/suheeryu/221314766979 - se_component_wrap 클래스명 사용\n",
    "# 신 주소 블로그 :  https://blog.naver.com/ultrabat/222670803753 - se-main-container 클래스명 사용\n",
    "\n",
    "print(\"=\" *80)\n",
    "print(\" 여러건의 블로그  크롤러 : 네이버 view -> 블로그 정보 수집하기\")\n",
    "print(\"=\" *80)\n",
    "\n",
    "#Step 1. 필요한 모듈과 라이브러리를 로딩합니다.\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import time\n",
    "import pandas  as pd    \n",
    "import math\n",
    "import urllib.request\n",
    "import urllib\n",
    "\n",
    "#Step 2. 필요한 정보 입력받기\n",
    "query_txt = input('1.정보를 수집할 키워드는 무엇입니까?: ')\n",
    "cnt = int(input('2.몇 건의 정보를 수집할까요? :'))\n",
    "page_cnt = math.ceil( cnt / 60 )\n",
    "f_dir = input(\"3.파일을 저장할 폴더명만 쓰세요(기본값:c:\\\\py_temp\\\\):\")\n",
    "if f_dir == '' :\n",
    "    f_dir=\"c:\\\\py_temp\\\\\"\n",
    "\n",
    "\n",
    "s_time = time.time( )\n",
    "\n",
    "#Step3. 검색어 입력한 후 검색하여 View로 이동하기\n",
    "s = Service(\"c:/py_temp/chromedriver.exe\")\n",
    "driver = webdriver.Chrome(service=s)\n",
    "\n",
    "url = 'https://www.naver.com'\n",
    "driver.get(url)\n",
    "driver.maximize_window( )\n",
    "time.sleep(2)\n",
    "\n",
    "element = driver.find_element(By.NAME,\"query\")\n",
    "element.send_keys(query_txt)\n",
    "element.send_keys('\\n')\n",
    "\n",
    "driver.find_element(By.LINK_TEXT,\"VIEW\").click()\n",
    "driver.find_element(By.LINK_TEXT,\"블로그\").click()\n",
    "time.sleep(1)\n",
    "\n",
    "# Step 4. 검색 요청 건수만큼 화면 스크롤링 하기\n",
    "# 자동 스크롤다운 함수\n",
    "def scroll_down(driver):\n",
    "    driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight);\")\n",
    "    time.sleep(3)\n",
    "\n",
    "if page_cnt > 2 :    \n",
    "    i = 1\n",
    "    while (i <= page_cnt+2):\n",
    "        scroll_down(driver) \n",
    "        i += 1\n",
    "        print('%s 페이지 정보를 추출하고 있으니 잠시만 기다려 주세요~~^^' %i)\n",
    "\n",
    "# Step 5. 현재 조회된 목록에서 URL 주소를 추출하여 리스트 생성하기\n",
    "url_all_list=[]    #조회할 블로그의 URL 정보 저장용 리스트\n",
    "\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "url_list_1 = soup.find('ul','lst_total').find_all('li')\n",
    "\n",
    "for a in url_list_1 :\n",
    "    url_all_list.append( a.find('div','total_area').find_all('a') )\n",
    "\n",
    "url_detail=[]\n",
    "for b in range(0,len(url_all_list)) :\n",
    "    url_detail.append( url_all_list[b][5]['href'] )\n",
    "\n",
    "url_final_list=[]   # 수집할 블로그 리스트를 저장할 변수\n",
    "no = 1\n",
    "for c in url_detail :    \n",
    "    if c.split('/')[2] == 'blog.naver.com' :\n",
    "        url_final_list.append(c)\n",
    "        no += 1\n",
    "\n",
    "        if no > cnt :\n",
    "            break\n",
    "\n",
    "d_no = 1\n",
    "print('')\n",
    "print('정보를 수집할 블로그 URL 주소는 아래와 같습니다~~~')\n",
    "print('')\n",
    "for d in url_final_list :   \n",
    "    print(d_no,':',d)\n",
    "    d_no += 1\n",
    "\n",
    "print('')\n",
    "\n",
    "#Step 6. 수집된 URL 주소에 접속하여 데이터 추출하기\n",
    "\n",
    "blog_addr2 = []\n",
    "w_name2 = []\n",
    "w_date2 = []\n",
    "blog_txt2 = []\n",
    "\n",
    "no = 1   # 전체 게시글 번호용 변수\n",
    "\n",
    "for blog_addr in url_final_list :            \n",
    "    driver.get(blog_addr)\n",
    "    driver.maximize_window( )\n",
    "    time.sleep(3)\n",
    "\n",
    "    driver.switch_to.frame('mainFrame')\n",
    "    \n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    addr_1 = soup.select('#postViewArea')   \n",
    "    addr_2 = soup.select('div[class=\"se_component_wrap sect_dsc __se_component_area\"]')   \n",
    "    addr_3 = soup.select('div[class=\"se-main-container\"]')  \n",
    "\n",
    "    if addr_1 :\n",
    "        img_no = 1\n",
    "        print()\n",
    "        print(\"유형 1이고 %s번째 게시글 정보를 수집합니다~~~~~~~~~~\" %no)\n",
    "        \n",
    "        # 블로그 URL 주소\n",
    "        print(\"1.블로그주소: \",blog_addr)\n",
    "        blog_addr2.append(blog_addr)\n",
    "        \n",
    "        # 작성자 닉네임\n",
    "        writer = soup.select(\"div.blog2_container > span.writer\")\n",
    "        try :\n",
    "                wname = writer[0].get_text( )   # 작성자 닉네임\n",
    "        except IndexError :\n",
    "                wname = \"작성자 닉네임이 없습니다\"\n",
    "        else :\n",
    "                wname = wname.replace(\"\\n\",\"\")\n",
    "\n",
    "        print(\"2.작성자 닉네임: \",wname )\n",
    "        w_name2.append(wname)\n",
    "        \n",
    "        # 작성일자\n",
    "        wdate = soup.select('p[class=\"date fil5 pcol2 _postAddDate\"]')\n",
    "        try : \n",
    "                wdate = wdate[0].get_text( )\n",
    "        except IndexError :\n",
    "                wdate = ' '\n",
    "\n",
    "        print(\"3.작성일자:\",wdate)\n",
    "        w_date2.append(wdate)\n",
    "\n",
    "        # 블로그 본문 내용\n",
    "        for i in addr_1:\n",
    "            blog_txt = i.text.replace(\"\\n\",\"\")\n",
    "            print(\"4.블로그내용: \\n\",blog_txt) # 엔터키를 제거하는 코드\n",
    "            print(\"\\n\")\n",
    "            blog_txt2.append(blog_txt)\n",
    "                \n",
    "        \n",
    "    elif addr_2 :\n",
    "        img_no =  1\n",
    "        print()\n",
    "        print(\"유형 2 이고 %s번째 게시글 정보를 수집합니다~~~~~~~~~~\" %no)\n",
    "        \n",
    "        # 블로그 주소\n",
    "        print(\"1.블로그주소: \",blog_addr)\n",
    "        blog_addr2.append(blog_addr)\n",
    "        \n",
    "        # 작성자 닉네임\n",
    "        writer = soup.select(\"div.blog2_container > span.writer\")\n",
    "        try :\n",
    "            wname = writer[0].get_text( )   # 작성자 닉네임\n",
    "        except IndexError :\n",
    "            wname = \"작성자 닉네임이 없습니다\"\n",
    "        else :\n",
    "            wname = wname.replace(\"\\n\",\"\")\n",
    "\n",
    "        print(\"2.작성자 닉네임: \",wname )\n",
    "        w_name2.append(wname)\n",
    "        \n",
    "        # 작성일자\n",
    "        wdate = soup.select(\"div.blog2_container > span.se_publishDate.pcol2\")\n",
    "        try : \n",
    "            wdate = wdate[0].get_text( )\n",
    "        except IndexError :\n",
    "            wdate = ' '\n",
    "\n",
    "        print(\"3.작성일자:\",wdate)\n",
    "        w_date2.append(wdate)\n",
    "\n",
    "        # 블로그 본문 내용\n",
    "        for i in addr_2:\n",
    "            blog_txt = i.text.replace(\"\\n\",\"\")\n",
    "            print(\"4.블로그내용: \\n\",blog_txt) \n",
    "            print(\"\\n\")\n",
    "            blog_txt2.append(blog_txt)\n",
    "                \n",
    "        \n",
    "    elif addr_3 :\n",
    "        img_no = 1\n",
    "        print()\n",
    "        print(\"유형 3이고 %s번째 게시글 정보를 수집합니다~~~~~~~~~~\" %no)\n",
    "        \n",
    "        # 블로그 주소\n",
    "        print(\"1.블로그주소: \",blog_addr)\n",
    "        blog_addr2.append(blog_addr)\n",
    "        \n",
    "        # 작성자 닉네임\n",
    "        writer = soup.select(\"div.blog2_container > span.writer\")\n",
    "        try :\n",
    "            wname = writer[0].get_text( )   # 작성자 닉네임\n",
    "        except IndexError :\n",
    "            wname = \"작성자 닉네임이 없습니다\"\n",
    "        else :\n",
    "            wname = wname.replace(\"\\n\",\"\")\n",
    "\n",
    "        print(\"2.작성자 닉네임: \",wname )\n",
    "        w_name2.append(wname)\n",
    "        \n",
    "        # 작성일자\n",
    "        wdate = soup.select(\"div.blog2_container > span.se_publishDate.pcol2\")\n",
    "        try : \n",
    "            wdate = wdate[0].get_text( )\n",
    "        except IndexError :\n",
    "            wdate = ' '\n",
    "\n",
    "        print(\"3.작성일자:\",wdate)\n",
    "        w_date2.append(wdate)\n",
    "        \n",
    "        #블로그 본문 내용\n",
    "        for i in addr_3:\n",
    "            blog_txt = i.text.replace(\"\\n\",\"\")\n",
    "            print(\"4.블로그내용: \\n\",blog_txt) \n",
    "            print(\"\\n\")\n",
    "            blog_txt2.append(blog_txt)\n",
    "                \n",
    "        \n",
    "    no += 1\n",
    "\n",
    "driver.close( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def maxprint( *a ) :\n",
    "    max = 0\n",
    "    for i in a :\n",
    "        if i > max :\n",
    "            max = i\n",
    "    return max\n",
    "\n",
    "maxprint(3,2,5,9,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
